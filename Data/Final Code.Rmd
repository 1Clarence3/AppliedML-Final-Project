---
date: "2025-05-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The main problem we're addressing is: “Which socioeconomic and housing-market factors most strongly influence county-level median housing-price growth, and how do those drivers vary across Census regions?”

```{R, warning = FALSE}

#1. Load Packages & Helpers 
required <- c(
  "tidyverse","skimr","naniar","corrplot",
  "caret","randomForest","MLmetrics",
  "tigris","sf","purrr","ggthemes"
)
new_pkgs  <- setdiff(required, rownames(installed.packages()))
if(length(new_pkgs)) install.packages(new_pkgs)
lapply(required, library, character.only=TRUE)
options(tigris_use_cache = TRUE)
```

#### Data Loading: We're loading a combined csv that has featuers for each US county from 2012 to 2022

```{r load-data}
data_path <- "updated_projectdata.csv"
df        <- read_csv(data_path)
cat("Rows × Cols:", dim(df), "\n")
glimpse(df)
```

#### The univariate EDA code below shows some basic statistics for certain important features we thought would be important 
```{r uni-eda}
skim(df %>% select(
  median_income, housing_price, permits, Affordability_Index
))

df %>% select(median_income, housing_price, Affordability_Index) %>%
  pivot_longer(everything(), names_to="var", values_to="val") %>%
  ggplot(aes(val)) +
    geom_histogram(bins=40, fill="skyblue", color="white") +
    facet_wrap(~var, scales="free") +
    labs(title="Key Variable Distributions")
```

#### Used correlation matrix to see relationships between variables (strength & directions of correlation). This way, if some features are too strongly correlated, we may drop them.

```{r corr-matrix}
num_df <- df %>%
  select(median_income, nonfamily_income, housing_price,
         permits, income_25_44, income_45_64,
         Income_to_Permits, Price_to_Permits,
         Affordability_Index)
M <- cor(num_df, use="pairwise.complete.obs")
corrplot(M, method="color", addCoef.col="black",
         tl.cex=0.7, title="Correlation Matrix", mar=c(0,0,1,0))

```

#### The first plot below shows average county-level housing price for each year. As you can see, it steadily increases. This might continue given current tariff/economic situation, which applies to our audience. To better visualize this increase, the second plot shows the average annual growth rate (% change in housing price relative to the prior year) of county-level housing prices over time. The results are more revealing here: after covid there was a sharp jump in price % change.

```{r time-trends-fixed}
df %>%
  group_by(year) %>%
  summarize(avg_price = mean(housing_price, na.rm = TRUE)) %>%
  ggplot(aes(x = factor(year), y = avg_price)) +  # Treat year as a factor
    geom_line(group = 1, color = "steelblue", size = 1) +  # group=1 is needed for line across factors
    geom_point(color = "steelblue", size = 2) +
    scale_y_continuous(labels = scales::dollar_format()) +
    labs(title = "Average County Housing Price by Year", x = "Year", y = "Avg. Price (USD)") +
    theme_minimal()

df %>%
  arrange(county, year) %>%
  group_by(county) %>%
  mutate(growth = (housing_price - lag(housing_price)) / lag(housing_price)) %>%
  ungroup() %>%
  group_by(year) %>%
  summarize(avg_growth = mean(growth, na.rm = TRUE)) %>%
  ggplot(aes(x = factor(year), y = avg_growth)) +  # Treat year as a factor
    geom_line(group = 1, color = "darkgreen", size = 1) +
    geom_point(color = "darkgreen", size = 2) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    labs(title = "Average Annual Housing Price Growth by Year", x = "Year", y = "Avg. Growth Rate") +
    theme_minimal()

```

#### GEO_ID, which is unique to each county, is used to extract state and county FIPS codes. We then rendered a map where each county is shaded by its housing price growth rate (spatial-choropleth graph)
```{r spatial-choropleth}
df_map <- df %>%
  mutate(
    GEO_chr     = as.character(GEO_ID),
    state_fips  = substr(GEO_chr, 1, 2),
    county_fips = substr(GEO_chr, 3, 5),
    GEOID       = paste0(state_fips, county_fips),
    growth_rate = (housing_price - lag(housing_price)) / lag(housing_price)
  ) %>% filter(!is.na(growth_rate))

counties_sf <- counties(cb=TRUE, year=2020, class="sf") %>% select(GEOID, geometry)
map_df <- left_join(counties_sf, df_map, by="GEOID")

ggplot(map_df) +
  geom_sf(aes(fill=growth_rate), color=NA) +
  scale_fill_viridis_c(labels=scales::percent_format()) +
  labs(title="County‐Level Annual Housing Price Growth", fill="Growth Rate") +
  theme_void() + coord_sf(expand = FALSE)

```

#### Lagged values are computed for housing price, income, population, and permits to enable calculation of year-over-year changes. The features we engineered were as follows:
#### Growth rate
#### price_income_ratio: captures housing affordability by comparing housing price to median income
#### pop_change: annual population growth as %
#### logs of price and income, which reduce skewness and enable modeling of relative changes
```{r feat-eng}
df <- df %>%
  arrange(county, year) %>%
  group_by(county) %>%
    mutate(
      price_lag           = lag(housing_price),
      growth_rate         = (housing_price - price_lag) / price_lag,
      median_inc_lag      = lag(median_income),
      pop_lag      = lag(population),
      permits_lag         = lag(permits),
      price_income_ratio  = housing_price / median_income,
       pop_change   = (population - pop_lag) / pop_lag,
      log_price           = log(housing_price),
      log_income          = log(median_income)
    ) %>%
  ungroup() %>% filter(!is.na(growth_rate))
```


#### Setting up features and ensuring cross validation is used for our 1st model below
```{r}
features <- c("pop_change","permits_lag")
ctrl     <- trainControl(method="cv", number=5)
```

### We first did a baseline model using linear regression. To simulate a baseline, we used 1 demand feature (pop_change) and 1 supply feature (permits_lag) to predict annual housing price growth
#### RMSE and MAE were used across cross-validation folds. Although the RMSE and MAE values are quite low, this model doesn't really help our audience given that young adults will likely consider more factors beyond population change and permits. 
```{r baseline-lm}
# filter out any rows with NA in the response or predictors
df_model <- df %>%
  drop_na(growth_rate, pop_change, permits_lag)

set.seed(42)
lm_baseline <- train(
  growth_rate ~ pop_change + permits_lag,
  data      = df_model,
  method    = "lm",
  trControl = ctrl,
  metric    = "RMSE"
)

# Report baseline performance
cat("Linear Baseline CV RMSE:", round(lm_baseline$results$RMSE,4), "  ")
cat("CV MAE:", round(lm_baseline$results$MAE,4), "\n")
```


#### Moving on from the baseline, we tried using random forest model with a broader set of engineered features as predictors (e.g., income ratios, log-transformed values).  Again, we trained the model with 5-fold cross-validation and used RMSE as our validation metric. After training, feature importances are extracted to identify which variables most strongly influence growth, and the top 10 are shown in a table, with the top 5 visualized in a bar chart
#### The reason for using this model was because based on the linear model above as well as the complx nature of real estate, we though there could be many complex and nonlinear relationships which random forests can handle
#### As for explaining the importance values in the bar graph, each decision tree in the forest is built by recursively splitting the data based on feature thresholds that best reduce a loss function. Here, the impurity is MSE and every time a feature is used to split a node, the reduction in MSE caused by that split is recorded. So basically this reduction is credited to the feature used for that split. Then across all trees these impurity reductions are summed for each feature which gives a raw importance score. We decided to normalize them out of 100 below.

```{r rf-cv}
features  <- c(
  "median_inc_lag","nonfamily_income","permits_lag",
  "income_25_44","income_45_64",
  "Income_to_Permits","Price_to_Permits",
  "price_income_ratio","log_price","log_income"
)
y <- df$growth_rate

set.seed(42)
ctrl      <- trainControl(method="cv", number=5)
rf_growth <- train(
  x = df[features], y = y,
  method    = "rf",
  trControl = ctrl,
  metric    = "RMSE",
  tuneLength= 3
)
rf_growth$resample %>%
  mutate(Fold = row_number()) %>%
  select(Fold, RMSE) %>%
  knitr::kable(digits=6, caption="RF CV RMSE for Growth")

#=== 10b. Random Forest Feature Importances ==================================
# What: rank drivers of growth; Why: understand which predictors matter most
imp_rf <- varImp(rf_growth)$importance %>%
  rownames_to_column(var = "Feature") %>%
  rename(Importance = Overall) %>%
  arrange(desc(Importance))

# Display top 10 features
imp_rf %>% head(10) %>% knitr::kable(digits=3, caption="Top 10 RF Feature Importances")

# Plot top 5 importances
ggplot(imp_rf[1:5, ], aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col(fill = "darkgreen") +
  coord_flip() +
  labs(title = "Top 5 Random Forest Predictors", y = "Importance (scaled)", x = NULL) +
  theme_minimal()

```

#### We first set up feature list and CV control for all models. Then we used ridge regression as another baseline leveraging penalized linear modeling. The same 5-fold CV was used. Unlike OLS like in the homeworks, ridge adds an L2 penalty (controlled by λ) that shrinks coefficients to reduce overfitting. We did that and set alpha=0 to specify Ridge (alpha = 1 would've been LASSO). A wide grid of lambda values was tested below to find the optimal level of regularization via cross-validation
#### We chose this model as it handles multicollinearity and provides a stronger parametric benchmark than mean or simple persistence
```{r}

features <- c(
  "median_inc_lag","nonfamily_income","permits_lag",
  "income_25_44","income_45_64",
  "Income_to_Permits","Price_to_Permits",
  "price_income_ratio","log_price","log_income"
)
ctrl <- trainControl(method="cv", number=5)

#Ridge regression
y <- df$growth_rate
ridge_grid <- expand.grid(alpha = 0,
                          lambda = 10^seq(-4, 1, length = 50))
set.seed(42)
ridge_baseline <- train(
  x          = df[features],
  y          = y,
  method     = "glmnet",
  preProcess = c("center","scale"),
  tuneGrid   = ridge_grid,
  trControl  = ctrl,
  metric     = "RMSE"
)
# Report baseline performance
cat("Ridge Baseline - best lambda:", ridge_baseline$bestTune$lambda, "
")
cat("Ridge Baseline CV RMSE:", round(min(ridge_baseline$results$RMSE),4), "
")
cat("Ridge Baseline CV MAE:", round(min(ridge_baseline$results$MAE, na.rm=TRUE),4), "
")
``` 

#### We compared the performances of all 3 models (Linear vs Ridge vs Random Forest) below. As you can see, the
```{r}


mod_results <- tibble(
  Model   = c("Linear Regression", "Ridge Baseline", "Random Forest"),
  RMSE_CV = c(
    min(lm_baseline$results$RMSE),
    min(ridge_baseline$results$RMSE),
    min(rf_growth$results$RMSE)
  )
)

# Display comparison table
knitr::kable(mod_results, digits=4, caption="CV RMSE: Linear vs Ridge vs RF")

# Bar plot of RMSE
ggplot(mod_results, aes(x=Model, y=RMSE_CV, fill=Model)) +
  geom_col() +
  labs(
    title = "Cross-Validation RMSE: Linear vs Ridge vs RF",
    y     = "RMSE",
    x     = "Model"
  ) +
  theme_minimal()

```


#### Now extending off of these results, it would be helpful to our audience if we analyzed the feature importances with temporal data. So what we did was fit a random forest model separately for each year to assess how feature importance changes over time. We then computed a mean importance for each feature across years and plotted the importance trends for the top 5 features with the highest average importance.

#### It is important to note the importance that was calculated/explained before using caret::varImp() is different from the one here. Now, with randomForest::importance(), this directly extracts raw mean decrease in MSE from the random forest model. I mentioned mean decrease in MSE instead of accuracy because in our case randomForest(x, y) is on aregression problem, and the mean decrease in MSE is reported per feature. This value reflects how much including that feature reduces prediction error, averaged across trees. Let's just take a concrete example: A value of 1.6 means that omitting that feature increases the mean squared error across trees by about 1.6 units (on average).

#### Now actually looking at the plot, the feature of median income jumps out as being very predictive of county-level housing price growth relative to the rest of the features. The other features of median income for young to middle-aged adults as well as the supply of permits are close in importance. And as for a more general trend, all the features jump in importance post-covid.

```{R}
years <- sort(unique(df$year))
imp_time <- map_df(years, function(yr){
  sub <- filter(df, year==yr)
  rf  <- randomForest(x=sub[features], y=sub$growth_rate)
  imp <- importance(rf)[,1]
  tibble(Year=yr, Feature=names(imp), Importance=imp)
})


top_feats <- imp_time %>%
  group_by(Feature) %>%
  summarize(mean_imp=mean(Importance)) %>%
  slice_max(mean_imp, n=5) %>%
  pull(Feature)

imp_time %>%
  filter(Feature %in% top_feats) %>%
  ggplot(aes(x = Year, y = Importance, color = Feature)) +
    geom_line(size = 1) + 
    geom_point() +
    scale_x_continuous(breaks = years) + 
    labs(title = "Top 5 Predictors: Importance Over Time", x = "Year") +
    theme_minimal()
```

#### Now we've looked at the features from above, and introduced how we got both the ridge and random forest importance values, the next logical step would be to compare the important features obtained from our different models. We then plotted the top 5 features (by importance) within each model.
```{r}
library(ggplot2)
library(tidytext)

# Ridge regression importances:
imp_ridge <- varImp(ridge_baseline)$importance %>%
  rownames_to_column("Feature") %>%
  rename(Importance = Overall) %>%
  mutate(Model = "Ridge")

# You already have RF importances in imp_rf, so just add a model column:
imp_rf2 <- imp_rf %>% mutate(Model = "RF")

# Combine all three:
imp_all <- bind_rows(imp_ridge, imp_rf2)

# Plot Top 5 per Model
top5_all <- imp_all %>%
  group_by(Model) %>%
  slice_max(order_by = Importance, n = 5) %>%
  ungroup()

ggplot(top5_all, aes(x = reorder_within(Feature, Importance, Model),
                     y = Importance, fill = Model)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ Model, scales = "free_y") +
  scale_x_reordered() +
  coord_flip() +
  labs(title = "Top 5 Predictors by Model (Coefficient‐based Importance)",
       x = NULL, y = "Importance (absolute, scaled)") +
  theme_minimal()
```


#### Having analyzed the important features from a temporal perspective, we want to close by addressing our question at the very beginning of how top predictors vary across U.S. regions since different regions may have distinct housing market dynamics

#### We did the following steps:
#### 1. Region Mapping: Map county FIPS codes to U.S. Census regions using the state FIPS prefix.
#### 2. Balanced Sampling: Balance each region's dataset by sampling the same number of counties (min_n) to avoid sample size bias. This esures fair comparison of variable importances across regions.
#### 3. Regional Random Forest Fits: For each region, we trained a separate Random Forest on the balanced sample, then extracted permutation-based variable importance (IncMSE = increase in MSE when feature is permuted). And just like before, collected importances for analysis.
#### 4. Plot: Lastly, we plooted the region-wise importance of the top 5 predictors (based on full-RF importances).

#### In terms of how we determined the 4 regions (midwest, northeast, south, west), we got the GEO_ID that starts with a 2-digit FIPS code representing the state and mapped them to the four standard U.S. Census Bureau regions manually as shown below. 

```{r}
# Spatial Variation of Importances (Balanced Sample)
# Baseline: We previously established a mean-growth baseline RMSE (~0.10) and overall Random Forest CV RMSE (~0.02). Here, we hold the model type and hyperparameters fixed (the RF trained per region uses the same features and default parameters) and focus on comparing variable importances across regions.
# Model: Random Forest regression (caret/RF) with balanced region-specific training sets.
 
# Manual FIPS→Region mapping
fips_to_region <- c(
  "01"="South","02"="West","04"="West","05"="South","06"="West",
  "08"="West","09"="Northeast","10"="South","11"="South","12"="South",
  "13"="South","15"="West","16"="West","17"="Midwest","18"="Midwest",
  "19"="Midwest","20"="Midwest","21"="South","22"="South","23"="Northeast",
  "24"="South","25"="Northeast","26"="Midwest","27"="Midwest","28"="South",
  "29"="Midwest","30"="West","31"="Midwest","32"="West","33"="Northeast",
  "34"="Northeast","35"="West","36"="Northeast","37"="South","38"="Midwest",
  "39"="Midwest","40"="South","41"="West","42"="Northeast","44"="Northeast",
  "45"="South","46"="Midwest","47"="South","48"="South","49"="West",
  "50"="Northeast","51"="South","53"="West","54"="South","55"="Midwest",
  "56"="West"
)

df_regions <- df %>%
  mutate(
    GEO_chr    = as.character(GEO_ID),
    state_fips = substr(GEO_chr, 1, 2),
    region     = fips_to_region[state_fips]
  )

# Filter to valid regions and balance sample sizes
region_counts <- df_regions %>%
  filter(!is.na(region)) %>%
  count(region, name="n")
min_n <- min(region_counts$n)
cat("Sampling", min_n, "rows per region for balanced RF fits.\n")

set.seed(42)
imp_reg_balanced <- map_df(region_counts$region, function(rgn) {
  sub      <- filter(df_regions, region == rgn)
  sub_samp <- sample_n(sub, min_n)
  rf       <- randomForest(x = sub_samp[features], y = sub_samp$growth_rate)
  im       <- importance(rf)[,1]
  tibble(
    Region     = rgn,
    Feature    = names(im),
    Importance = im
  )
})

# Plot balanced importances for top features
imp_reg_balanced %>%
  filter(Feature %in% imp_rf$Feature[1:5]) %>%
  ggplot(aes(x = Region, y = Importance, fill = Region)) +
    geom_col(position = "dodge") +
    facet_wrap(~ Feature, scales = "free_y") +
    labs(
      title = "Top Predictors of Price Growth by Region (Balanced Samples)",
      y     = "Importance (IncMSE)"
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
